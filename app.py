import streamlit as st
import google.generativeai as genai
import fitz
from dotenv import load_dotenv
import os
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

# ============ INIT ===================
load_dotenv()                       # laddar in .env filen 
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))    # Startar upp Gemini API och anv√§nder min nyckel sparad i .env filen

embedding_model = SentenceTransformer("all-MiniLM-L6-v2") # Anv√§nder mig av en f√∂rtr√§nad spr√•kmodel

# ============ PDF -> CHUNKS ===================
# Min funktion "extract text from pdf" som tar in s√∂kv√§gen till mina pdfer som ligger i min project directory
def extract_text_from_pdf(pdf_path):
    text = ""
    doc = fitz.open(pdf_path)
    for page in doc:
        text += page.get_text()
    return text

# H√§r anv√§nder jag mig av funktionen som jag deklarerade ovanf√∂r och l√§ser in mina dokument, i detta fall mina 6 lagar inom arbetsr√§tt
pdf_texts = [
    extract_text_from_pdf("pdfs/AnstallningsskyddLag.pdf"),
    extract_text_from_pdf("pdfs/Arbetstidslag.pdf"),
    extract_text_from_pdf("pdfs/MedbestammandeLag.pdf"),
    extract_text_from_pdf("pdfs/Offentliganstallning.pdf"),
    extract_text_from_pdf("pdfs/RattegangArbetstvist.pdf"),
    extract_text_from_pdf("pdfs/SemesterLag.pdf"),
]

# Kombinerar alla mina 6 lag texter till en l√•ng textstr√§ng f√∂r att sedan kunna dela upp den i chunks
all_text = "\n\n".join(pdf_texts)

# Nu skapar jag en funktion f√∂r att dela upp hela textstr√§ngen i mindre bitar (max 1000 char) och detta g√∂rs pga begr√§nsat minne i Gemini
def split_text_into_chunks(text, max_length=1000):
    chunks = []
    current_chunk = ""

    for paragraph in text.split("\n"):
        paragraph = paragraph.strip()
        if not paragraph:
            continue

        if len(current_chunk) + len(paragraph) < max_length:
            current_chunk += paragraph + "\n"
        else:
            chunks.append(current_chunk.strip())
            current_chunk = paragraph + "\n"

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

# H√§r anv√§nder jag mig av funktionen och applicerar den p√• hela kombinerade textstr√§ngen
chunks = split_text_into_chunks(all_text)

# ============ EMBEDDINGS & INDEX ===================
# Jag g√∂r om alla chunks till vektorer
embeddings = embedding_model.encode(chunks, convert_to_numpy=True)
index = faiss.IndexFlatL2(embeddings.shape[1]) #skapar ett s√∂kbart index √∂ver v√•ra vektorer
index.add(embeddings) #l√§gger till alla vektorer i indexet

# ============ FUNKTIONER ===================
#H√§r kopplar jag ihop anv√§ndarens fr√•ga med lagtexten f√∂r att skapa ett relevant svar.
# Jag g√∂r detta genom att skapa en embedding av fr√•gan och s√∂ka i FAISS-indexet efter liknande chunkar.

def retrieve_relevant_chunks(query, model, index, chunk_lookup, k=3):
    query_embedding = model.encode([query])
    D, I = index.search(np.array(query_embedding), k)
    return [chunk_lookup[i] for i in I[0]]

#Skapar en prompt d√§r de mest relevanta lagtext-chunkarna samt fr√•gan skickas till Gemini.
#Modellen anv√§nder detta f√∂r att generera ett juridiskt svar baserat p√• kontext.
def generate_gemini_answer(question, context_chunks):
    context = "\n\n".join(context_chunks)
    prompt = f"""
Du √§r en juridisk assistent med fokus p√• svensk arbetsr√§tt.
Anv√§nd f√∂ljande utdrag ur lagtexten f√∂r att svara p√• fr√•gan.

LAGTEXT:
{context}

FR√ÖGA:
{question}

SVAR:"""
    model = genai.GenerativeModel("gemini-1.5-flash") #Initierar Gemini-modellen f√∂r att generera svar
    response = model.generate_content(prompt)
    return response.text

# ============ STREAMLIT UI ===================
#Skapar Streamlit sidan f√∂r att kunna visualisera/presentera chatbotten och ge anv√§ndaren en m√∂jlighet att interagera med botten.
st.set_page_config(page_title="Svensk Juridikbot", layout="centered")
st.title("‚öñÔ∏è Svensk Juridikbot")
st.markdown("St√§ll en fr√•ga relaterad till svensk arbetsr√§tt:")

user_question = st.text_input("Din fr√•ga", placeholder="Vad g√§ller vid upps√§gning p√• grund av arbetsbrist?")

if user_question:
    with st.spinner("H√§mtar relevant lagtext..."):
        relevant_chunks = retrieve_relevant_chunks(user_question, embedding_model, index, chunks)

    with st.spinner("Genererar svar fr√•n Gemini..."):
        response = generate_gemini_answer(user_question, relevant_chunks)

    st.subheader("üßæ Svar")
    st.write(response)

    st.markdown("üîç *K√§llor (relevanta utdrag):*")
    for i, c in enumerate(relevant_chunks):
        st.markdown(f"**{i+1}.** {c}")

    # ======= Feedbacksystem ==========
    # Evalueringssystem d√§r anv√§ndaren kan ange 1-5 beroende p√• hur bra svaret var. Feedback sparas i .txt fil
    st.markdown("### ‚≠ê Hur n√∂jd √§r du med svaret?")
    rating = st.slider("Betygs√§tt svaret (1 = d√•ligt, 5 = utm√§rkt)", 1, 5, 3)

    if st.button("Skicka feedback"):
        with open("feedback_log.txt", "a", encoding="utf-8") as f:
            f.write(f"Fr√•ga: {user_question}\n")
            f.write(f"Svar: {response}\n")
            f.write(f"Betyg: {rating}\n")
            f.write("-" * 40 + "\n")
        st.success("Tack f√∂r din feedback!")
